import os
import os.path as osp
import csv
import argparse
from collections import defaultdict, OrderedDict


def get_run_size_from_filename(filename):
    # bubble_a_1000.csv -> bubble_a_1000
    run_name = osp.splitext(filename)[0]

    # bubble_a_1000 -> 1000
    run_size = int(run_name.rsplit('_', 1)[1])

    return run_size


def get_avg_stddev(folder, filename):
    filename = osp.join(folder, filename)

    with open(filename) as csv_file:
        file_rows = csv.reader(csv_file, delimiter=',')

        for rr in file_rows:
            row_size = len(rr)
            csv_file.close()
            return rr[row_size-2], rr[row_size-1]


if __name__ == '__main__':
    # OBS: This program trusts that you're using it with the reports generated by the runner utility.

    parser = argparse.ArgumentParser("Summarises all the generated statistics by the runner script, assuming tat this"
                                     "data is stored under the /reports folder.",
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-g', '--graph', help="If this flag is set, then a graph will be generated for each algorithm"
                        "benchmark. Notice that you should have matplotlib properly installed on your environment.",
                        action='store_true', dest='graph')

    args = parser.parse_args()

    reports_dir = osp.join(os.getcwd(), 'reports')
    filenames = os.listdir(reports_dir)

    runs = defaultdict(list)

    # discovering reports file names
    for filename in filenames:
        # bubble_a_1000.csv -> bubble_a_1000
        run_name = osp.splitext(filename)[0]
        # bubble_a_1000 -> bubble_a
        algorithm_name = run_name.rsplit('_', 1)[0]

        runs[algorithm_name].append(filename)

    # sorting runs in ascending order by batch size
    for algorithm in runs.keys():
        runs[algorithm] = sorted(runs[algorithm], key=lambda x: get_run_size_from_filename(x))

    # compiling average and std_dev for each algorithm execution mode
    execution_statistics = defaultdict(dict)

    for algorithm, filenames in runs.items():
        sizes = [get_run_size_from_filename(filename) for filename in filenames]

        average_time = list()
        devs = list()

        for filename in filenames:
            # run_size = get_run_size_from_filename(filename)
            avg, stddev = get_avg_stddev(reports_dir, filename)

            average_time.append(avg)
            devs.append(stddev)

        data = {'size': sizes, 'average_time': average_time, 'std_dev': devs}

        # quick_a -> [quick, a]
        algo_name, mode = algorithm.rsplit('_', 1)

        # storing everything into the lookup table
        execution_statistics[algo_name][mode] = data

    # writing execution summary into a separate CSV file.
    subtitle = {'r': 'random', 'a': 'ascending', 'd': 'descending'}
    with open('summary.csv', 'w') as file:
        csv_writer = csv.writer(file, delimiter=',')

        quadratic = dict()
        log_linear = dict()

        for algorithm, statistics in execution_statistics.items():
            for mode, statistic in statistics.items():

                header = '%s - %s' % (algorithm, subtitle[mode])
                csv_writer.writerow([header])

                csv_writer.writerow(statistic['size'])
                csv_writer.writerow(statistic['average_time'])
                csv_writer.writerow(statistic['std_dev'])
                csv_writer.writerow(list())

                if mode == 'r':
                    dataset_size = len(statistic['size'])

                    if dataset_size == 7:
                        quadratic[algorithm] = statistic
                    else:
                        log_linear[algorithm] = statistic

        file.close()

    if args.graph:
        from grapher import grapher
        import matplotlib
        import numpy as np
        from collections import OrderedDict

        # creates entries to be used when plotting the summary graphs
        def make_entry(algorithm, leg_name, color=None):
            return {
                algorithm: {
                    'name': leg_name,
                    'color': color
                }
            }

        # plots individual graphs
        for algorithm, statistics in execution_statistics.items():
            print(algorithm)
            grapher.plot(algorithm, statistics)

        # plots quick_sort vs iterative_quick_sort
        quick = execution_statistics['quick']
        iquick = execution_statistics['iquick']

        grapher.plot_a_vs_b(quick, iquick)

        # now plots all vs all
        tt = ['#00aedb', '#a32294', '#8ec127', '#f47835', '#00d5e0', '#FF0000', '#0000FF', '#306db7', '#ff5d00',
              '#77547c', '#9e9a8a']

        del log_linear['quick']
        del log_linear['quick_median']

        # r'$...$ creates a LaTeX string (spaces should be created with \:)
        output_mapping = OrderedDict()

        output_mapping.update(make_entry('bubble', 'Bubble sort', tt[0]))
        output_mapping.update(make_entry('bubble_improved', r'$Bubble\:sort^{2}$', tt[1]))
        output_mapping.update(make_entry('insertion', 'Insertion sort', tt[2]))
        output_mapping.update(make_entry('selection', 'Selection sort', tt[3]))

        output_mapping.update(make_entry('merge', 'Merge sort', tt[4]))
        output_mapping.update(make_entry('heap', 'Heap sort', tt[5]))
        output_mapping.update(make_entry('shell', 'Shell sort', tt[6]))

        # output_mapping.update(make_entry('quick', 'Quick sort', tt[7]))
        output_mapping.update(make_entry('iquick', r'$Quick\:sort^{1}$', tt[8]))
        output_mapping.update(make_entry('iquick_median', r'$Quick\:sort^{1}$', tt[9]))
        # output_mapping.update(make_entry('quick_median', r'$Quick\:sort^{2}$', tt[10]))

        grapher.plot_many_vs_many(quadratic, 'quadratics', output_mapping)
        grapher.plot_many_vs_many(log_linear, 'log_linear', output_mapping)

        log_linear.update(quadratic)
        grapher.plot_many_vs_many(log_linear, 'all', output_mapping, True)
